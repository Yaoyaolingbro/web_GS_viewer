# splat (中文版)

这是一个 WebGL 实现的实时渲染器，用于[3D 高斯泼溅的实时辐射场渲染](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)。这是一种最近开发的技术，可以通过一组图片生成一个可导航的照片级真实感 3D 场景。由于它本质上是点云渲染的扩展，使用这种技术生成的场景可以在普通图形硬件上非常高效地渲染——这与之前的类似技术（如 NeRF）不同。

如果你在浙大校园网内，你可以使用以下链接直接访问：
- https://10.71.106.9:8000

> 申明，本项目参考了[splat](https://github.com/antimatter15/splat)在此基础上做了一些更编写的操作。考虑后续作为js的实践添加更多加速功能，尽可能完善目前透明度渲染的问题。


## 控制说明

移动 (方向键 & Shift/空格键)

- `左`/`右` 方向键：左右平移
- `上`/`下` 方向键：前进/后退
- `Shift` 键：向上移动
- `空格` 键：向下移动

相机视角 (WASD键)

- `A`/`D` 键：左/右转动相机
- `W`/`S` 键：上/下倾斜相机
- `Q`/`E` 键：逆时针/顺时针滚动相机 (旋转幅度已减小)
- `I`/`K` 和 `J`/`L` 键：环绕观察

触摸板
- 上/下滚动：向下环绕观察
- 左/右滚动：向左/右环绕观察
- 双指捏合：前进/后退
- `Ctrl` 键 + 上/下滚动：前进/后退
- `Shift` 键 + 上/下滚动：向上/下移动
- `Shift` 键 + 左/右滚动：左右平移

鼠标
- 点击并拖动：环绕观察
- 右键点击 (或 `Ctrl`/`Cmd` 键) 并上/下拖动：前进/后退
- 右键点击 (或 `Ctrl`/`Cmd` 键) 并左/右拖动：左右平移

触摸操作 (移动设备)
- 单指操作：环绕观察
- 双指捏合：前进/后退
- 双指旋转：顺时针/逆时针旋转相机
- 双指平移：左右和上下移动

其他
- 按 `0-9` 数字键：切换到预加载的相机视角之一
- 按 `-` 或 `+` 键：循环切换已加载的相机
- 按 `P` 键：恢复默认动画
- 拖放 `.ply` 文件：转换为 `.splat` 格式
- 拖放 `cameras.json` 文件：加载相机视角

## 其他功能

- 按 `V` 键：将当前视图坐标保存到 URL 中
- 通过向启用了 CORS 的 URL 添加 `url` 参数来打开自定义的 `.splat` 文件
- 将使用 3D 高斯泼溅软件处理过的 `.ply` 文件拖放到页面上，它将自动将文件转换为 `.splat` 格式

## 示例
使用`run.sh`即可开启你自己的服务器，或者直接使用`python -m http.server`命令开启一个简单的服务器。



## 注意事项

- 使用 JavaScript 和 WebGL 1.0 编写，无外部依赖。你可以直接查看源代码阅读未压缩的代码。WebGL 2.0 并没有真正添加任何 WebGL 1.0 扩展无法实现的新功能。WebGPU 据说不错，但在 Chromium 之外的支持仍然不太好。
- 我们通过结合大小和不透明度对 splat 进行排序，并支持渐进式加载，这样你可以在所有 splat 加载完成之前查看并与模型交互。
- 目前不支持使用球谐函数实现视角相关的着色效果，这主要是为了减小 splat 格式的文件大小，以便可以轻松加载到 Web 浏览器中。对于三阶球谐函数，我们需要 48 个系数，每个 splat 几乎需要 200 字节！
- Splat 排序是在 WebWorker 中的 CPU 上异步完成的。研究在 GPU 上使用位排序或基数排序实现排序可能会很有趣，但在我看来，让 GPU 专注于渲染而不是在渲染和排序之间分配时间可能更好。
- 早期的实验使用了[随机透明度](https://research.nvidia.com/publication/2011-08_stochastic-transparency)（看起来有颗粒感）和[加权混合顺序无关透明度](https://learnopengl.com/Guest-Articles/2020/OIT/Weighted-Blended)（似乎不起作用）。

## words

gaussian splats are very efficient to render because they work in a way which is very similar to point clouds— in fact they use the same file format (`.ply`) and open them up with the same tools (though to see colors in meshlab, you should convert the spherical harmonic zeroth order terms into rgb colors first). you can think of them as essentially generalizing individual points into translucent 3D blobs (the eponymous splats). 

that said, even though the inference process is very similar to a traditional 3d rendering, the reference implementation doesn't leverage any of that because for training it needs the entire render pipeline to be differentiable (i.e. you need to be able to run the rendering process "backwards" to figure out how to wiggle the location, size and color of each blob to make a particular camera's view incrementally closer to that of a reference photograph). whether or not this gradient based optimization counts as neural is i guess a somewhat debated question online.

since this implementation is just a viewer we don't need to do any differentiable rendering. our general approach is to take each splat and feed it into a vertex shader. we take the xyz position of the splat and project it to the screen coordinates with a projection matrix, and we take the scale and quaternion rotation parameters of the splat and figure out the projected eigenvectors so we can draw a bounding quadrilateral. these quadrilaterals are then individually shaded with a fragment shader. 

the fragment shader is a program which essentially runs for each pixel on each fragment (i.e. quadrilateral that was generated by the vertex shader) and outputs a color. It takes its position, calculates the distance from the center of the splat and uses it to determine the opacity channel of the splat's color. right now this implementation only stores 3 (red, blue, green) channels of color for a splat, but the full implementation uses essentially 48 channels to encode arbitrary view-dependent lighting. 

the most annoying problem comes with how these fragments come together and create an actual image. it turns out that rendering translucent objects in general is a somewhat unsolved problem in computer graphics which ultimately stems from the fact that compositing translucent things is not commutative, i.e. a stack of translucent objects looks different based on the order in which they are drawn. 

one easy solution is called speculative transparency, where basically you pretend that you actually have no translucency at all- objects are just different levels of randomized swiss cheese. the graphics card keeps track of a z-buffer and discards all the pixels which are not the top-most, and we generate a random number at each pixel and then discard it if it 90% of the time if it is 90% transparent. this works but it gives everything a noisy, dithered look.

another easy approach is to use the painter's algorithm, which basically involves pre-sorting all your objects before rendering them. doing this on the CPU can be rather expensive, with the ~1M splats on the demo page, it takes about 150ms to sort through them all on my computer. 

the approach that the reference implementation, and most other implementations of gaussian splatting take is to do the sorting on the GPU. one common algorithm for doing sorts on the gpu is called the [bitonic sort](https://en.wikipedia.org/wiki/Bitonic_sorter) as it is very parallelizable. a normal cpu comparison sorting algorithm like quicksort/mergesort can run in O(n log n) time, the bitonic sort is a bit slower at O(n log^2 n), but the n factor can be done in parallel, so the overall latency is O(log^2 n) which is faster than than O(n log n). the reference implementation uses a radix sort based on [onesweep](https://arxiv.org/abs/2206.01784), which can happen in O(n) time because you can leverage the fact that you're sorting numbers to get more information at each cycle than a single comparison. 

chrome has recently shipped webgpu, which is a new very clean api that apparently makes it possible to write things like compute shaders similar to CUDA that work in the browser. however, webgpu is not yet supported by firefox and safari. this means that if we want to build something that is broadly usable, we have to stick with the older webgl (and maybe even webgl 1.0, since there are reports that webgl 2.0 is buggy or slow on safari with the new M1 chips). It's still probably possible to implement a bitonic sort on top of webgl, but it would take about 200 iterations to sort 1M numbers, so it might still be too slow. 

another approach to rendering translucent objects is called depth peeling, where you enable the z-buffer and only render the translucent objects that are on the top, and then feed that z-buffer back into the render process to "peel" off the top and render only the layer beneath, before stacking those translucent layers together to a final image. I didn't manage to get this to work, but it's likely that it would be slow anyway.

another interesting approach is something called [weighted blended order independent transparency](https://learnopengl.com/Guest-Articles/2020/OIT/Weighted-Blended) which adds an additional number saved to a different render buffer which is used as a weight for an approximation of translucency which is commutative. it didn't work in my experiments, which is somewhat expected in situations where you have certain splats with high opacity on top of each other. 

the final approach that i settled on is to run the sorting process on the CPU in a webworker, which happens a bit more slowly (at roughly 4fps whereas the main render is at 60fps), but that's fine because most of the time when you are moving around the z order doesn't actually change very fast (this results in momentary artifacts when jumping directly between different camera orientations on opposite sides). 


## acknowledgements

Thanks to Otavio Good for discussions on different approaches for [order independent transparency](https://en.wikipedia.org/wiki/Order-independent_transparency), Mikola Lysenko for [regl](http://regl.party/) and also for helpful advice about webgl and webgpu, Ethan Weber for discussions about how NeRFs work and letting me know that sorting is hard, Gray Crawford for identifying issues with color rendering and camera controls, Anna Brewer for help with implementing animations, and GPT-4 for writing all the WebGL boilerplate.
